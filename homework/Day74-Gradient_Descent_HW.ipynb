{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業目標:\n",
    "    \n",
    "    了解數學式與利用超參數調整求導梯度下降的過程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業重點:\n",
    "\n",
    "(1)dfunc 是 func 偏微分的公式，X^2 偏微分等於 2 * X，可以同時改變 func、dfunc 內容\n",
    "\n",
    "(2)調整其它 Hyperparameters: w_init、epochs、lr、decay、momentom測試逼近的過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "# 目標函數:y=(x+5)^2\n",
    "\n",
    "# 目標函數一階導數:dy/dx=2*(x+5)\n",
    "\n",
    "'''\n",
    "\n",
    "def GD(w_init, df, epochs, lr):    \n",
    "    \"\"\"  梯度下降法。給定起始點與目標函數的一階導函數，求在epochs次反覆運算中x的更新值\n",
    "        :param w_init: w的init value    \n",
    "        :param df: 目標函數的一階導函數    \n",
    "        :param epochs: 反覆運算週期    \n",
    "        :param lr: 學習率    \n",
    "        :return: x在每次反覆運算後的位置   \n",
    "     \"\"\"    \n",
    "    xs = np.zeros(epochs+1) # 把 \"epochs+1\" 轉成dtype=np.float32    \n",
    "    x = w_init    \n",
    "    xs[0] = x    \n",
    "    for i in range(epochs):         \n",
    "        dx = df(x)        \n",
    "        # v表示x要跨出的幅度        \n",
    "        v = - dx * lr        \n",
    "        x += v        \n",
    "        xs[i+1] = x    \n",
    "    return xs\n",
    "\n",
    "# 起始權重\n",
    "w_init = 3    \n",
    "# 執行週期數\n",
    "epochs = 20 \n",
    "# 學習率   \n",
    "lr = 0.3   \n",
    "# 梯度下降法 \n",
    "x = GD(w_init, dfunc, epochs, lr=lr) \n",
    "print (x)\n",
    "\n",
    "#劃出曲線圖\n",
    "color = 'r'    \n",
    " \n",
    "from numpy import arange\n",
    "t = arange(-6.0, 6.0, 0.01)\n",
    "plt.plot(t, func(t), c='b')\n",
    "plt.plot(x, func(x), c=color, label='lr={}'.format(lr))    \n",
    "plt.scatter(x, func(x), c=color, )    \n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 學習率對梯度下降法的影響 \n",
    "學習率較小時，收斂到正確結果的速度較慢。\n",
    "學習率較大時，容易在搜索過程中發生震盪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_x = np.linspace(-5, 5, 100)\n",
    "line_y = func(line_x)\n",
    "plt.figure('Gradient Desent: Learning Rate')\n",
    "\n",
    "'''\n",
    "w_init\n",
    "epochs \n",
    "x = w_init\n",
    "lr = [........]\n",
    "'''\n",
    "\n",
    "color = ['r', 'g', 'y']\n",
    "size = np.ones(epochs+1) * 10\n",
    "size[-1] = 70\n",
    "for i in range(len(lr)):\n",
    "    x = GD(w_init, dfunc, epochs, lr=lr[i])\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.plot(line_x, line_y, c='b')\n",
    "    plt.plot(x, func(x), c=color[i], label='lr={}'.format(lr[i]))\n",
    "    plt.scatter(x, func(x), c=color[i])\n",
    "    plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "學習率較大時，容易在搜索過程中發生震盪，而發生震盪的根本原因無非就是搜索的步長邁的太大了\n",
    "如果讓能夠lr隨著迭代週期不斷衰減變小，那麼搜索時邁的步長就能不斷減少以減緩震盪學習率衰減因子由此誕生"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 學習率衰減公式\n",
    "\n",
    "lr_i = lr_start * 1.0 / (1.0 + decay * i)\n",
    "\n",
    "\n",
    "其中lr_i為第一迭代i時的學習率，lr_start為原始學習率，decay為一個介於[0.0, 1.0]的小數。從公式上可看出：\n",
    "\n",
    "decay越小，學習率衰減地越慢，當decay = 0時，學習率保持不變。\n",
    "decay越大，學習率衰減地越快，當decay = 1時，學習率衰減最快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_decay(w_init, df, epochs, lr, decay):\n",
    "    xs = np.zeros(epochs+1)\n",
    "    x = w_init\n",
    "    xs[0] = x\n",
    "    v = 0\n",
    "    for i in range(epochs):\n",
    "        dx = df(x)\n",
    "        # 學習率衰減 \n",
    "        lr_i = lr * 1.0 / (1.0 + decay * i)\n",
    "        # v表示x要改变的幅度\n",
    "        v = - dx * lr_i\n",
    "        x += v\n",
    "        xs[i+1] = x\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_x = np.linspace(-5, 5, 100)\n",
    "line_y = func(line_x)\n",
    "plt.figure('Gradient Desent: Decay')\n",
    "\n",
    "lr = 1.0\n",
    "iterations = np.arange(300)\n",
    "decay = [0.0, 0.001, 0.1, 0.5, 0.9, 0.99]\n",
    "for i in range(len(decay)):\n",
    "    decay_lr = lr * (1.0 / (1.0 + decay[i] * iterations))\n",
    "    plt.plot(iterations, decay_lr, label='decay={}'.format(decay[i]))\n",
    "\n",
    "plt.ylim([0, 1.1])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "衰減越大，學習率衰減地越快。\n",
    "衰減確實能夠對震盪起到減緩的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum (動量)\n",
    "如何用“動量”來解決:\n",
    "\n",
    "(1)學習率較小時，收斂到極值的速度較慢。\n",
    "\n",
    "(2)學習率較大時，容易在搜索過程中發生震盪。\n",
    "\n",
    "當使用動量時，則把每次w的更新量v考慮為本次的梯度下降量 (-dx*lr), 與上次w的更新量v乘上一個介於[0, 1]的因子momentum的和\n",
    "\n",
    "\n",
    "w ← x − α ∗ dw (x沿負梯度方向下降)\n",
    "\n",
    "v =  ß ∗ v − α  ∗ d w\n",
    "\n",
    "w ← w + v\n",
    "\n",
    "(ß 即momentum係數，通俗的理解上面式子就是，如果上一次的momentum（即ß ）與這一次的負梯度方向是相同的，那這次下降的幅度就會加大，所以這樣做能夠達到加速收斂的過程 \n",
    "\n",
    "如果上一次的momentum（即ß ）與這一次的負梯度方向是相反的，那這次下降的幅度就會縮減，所以這樣做能夠達到減速收斂的過程 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_x = np.linspace(-5, 5, 100)\n",
    "line_y = func(line_x)\n",
    "plt.figure('Gradient Desent: Decay')\n",
    "\n",
    "'''\n",
    "x= w_init\n",
    "epochs = 10\n",
    "\n",
    "lr = [.......]\n",
    "decay = [.......]\n",
    "'''\n",
    "\n",
    "color = ['k', 'r', 'g', 'y']\n",
    "\n",
    "row = len(lr)\n",
    "col = len(decay)\n",
    "size = np.ones(epochs + 1) * 10\n",
    "size[-1] = 70\n",
    "for i in range(row):\n",
    "     for j in range(col):\n",
    "        x = GD_decay(x_start, dfunc, epochs, lr=lr[i], decay=decay[j])\n",
    "        plt.subplot(row, col, i * col + j + 1)\n",
    "        plt.plot(line_x, line_y, c='b')\n",
    "        plt.plot(x, func(x), c=color[i], label='lr={}, de={}'.format(lr[i], decay[j]))\n",
    "        plt.scatter(x, func(x), c=color[i], s=size)\n",
    "        plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
