{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "print(os.listdir(\"./data-science-london-scikit-learn\"))\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# read csv (comma separated value) into data\n",
    "train = pd.read_csv('./data-science-london-scikit-learn/train.csv', header=None)\n",
    "trainLabel = pd.read_csv('./data-science-london-scikit-learn/trainLabels.csv', header=None)\n",
    "test = pd.read_csv('./data-science-london-scikit-learn/test.csv', header=None)\n",
    "\n",
    "# look at available plot styles\n",
    "print(plt.style.available) \n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "print('train shape:', train.shape)\n",
    "print('test shape:', test.shape)\n",
    "print('trainLabel shape:', trainLabel.shape)\n",
    "train.head()\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "train.info()\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "train.describe()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# KNN with cross-validation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "X, y = train, np.ravel(trainLabel)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# Model complexity\n",
    "neig = np.arange(1, 25)\n",
    "kfold = 10\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "bestKnn = None\n",
    "bestAcc = 0.0\n",
    "\n",
    "# Loop over different values of k\n",
    "for i, k in enumerate(neig):\n",
    "    # k from 1 to 25(exclude)\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # Fit with knn\n",
    "    knn.fit(X_train,y_train)\n",
    "    \n",
    "    #train accuracy\n",
    "    train_accuracy.append(knn.score(X_train, y_train))\n",
    "    \n",
    "    # test accuracy\n",
    "    val_accuracy.append(np.mean(cross_val_score(knn, X, y, cv=kfold)))\n",
    "    if np.mean(cross_val_score(knn, X, y, cv=kfold)) > bestAcc:\n",
    "        bestAcc = np.mean(cross_val_score(knn, X, y, cv=10))\n",
    "        bestKnn = knn\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[13,8])\n",
    "plt.plot(neig, val_accuracy, label = 'Validation Accuracy')\n",
    "plt.plot(neig, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.title('k value VS Accuracy')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(neig)\n",
    "plt.show()\n",
    "\n",
    "print('Best Accuracy without feature scaling:', bestAcc)\n",
    "print(bestKnn)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "# predict test\n",
    "test_fill = np.nan_to_num(test)\n",
    "submission = pd.DataFrame(bestKnn.predict(test_fill))\n",
    "print(submission.shape)\n",
    "submission.columns = ['Solution']\n",
    "submission['Id'] = np.arange(1,submission.shape[0]+1)\n",
    "submission = submission[['Id', 'Solution']]\n",
    "submission\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "submission.to_csv('./data-science-london-scikit-learn/submission_no_normalization.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # ## Add Feature Scaling\n",
    "\n",
    "# # In[16]:\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "\n",
    "# std = StandardScaler()\n",
    "# X_std = std.fit_transform(X)\n",
    "# mms = MinMaxScaler()\n",
    "# X_mms = mms.fit_transform(X)\n",
    "# norm = Normalizer()\n",
    "# X_norm = norm.fit_transform(X)\n",
    "\n",
    "\n",
    "# # In[17]:\n",
    "\n",
    "\n",
    "# # Model complexity\n",
    "# neig = np.arange(1, 30)\n",
    "# kfold = 10\n",
    "# val_accuracy = {'std':[], 'mms':[], 'norm':[]}\n",
    "# bestKnn = None\n",
    "# bestAcc = 0.0\n",
    "# bestScaling = None\n",
    "# # Loop over different values of k\n",
    "# for i, k in enumerate(neig):\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#     # validation accuracy\n",
    "#     s1 = np.mean(cross_val_score(knn, X_std, y, cv=kfold))\n",
    "#     val_accuracy['std'].append(s1)\n",
    "#     s2 = np.mean(cross_val_score(knn, X_mms, y, cv=kfold))\n",
    "#     val_accuracy['mms'].append(s2)\n",
    "#     s3 = np.mean(cross_val_score(knn, X_norm, y, cv=kfold))\n",
    "#     val_accuracy['norm'].append(s3)\n",
    "#     if s1 > bestAcc:\n",
    "#         bestAcc = s1\n",
    "#         bestKnn = knn\n",
    "#         bestScaling = 'std'\n",
    "#     elif s2 > bestAcc:\n",
    "#         bestAcc = s2\n",
    "#         bestKnn = knn\n",
    "#         bestScaling = 'mms'\n",
    "#     elif s3 > bestAcc:\n",
    "#         bestAcc = s3\n",
    "#         bestKnn = knn\n",
    "#         bestScaling = 'norm'\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=[13,8])\n",
    "# plt.plot(neig, val_accuracy['std'], label = 'CV Accuracy with std')\n",
    "# plt.plot(neig, val_accuracy['mms'], label = 'CV Accuracy with mms')\n",
    "# plt.plot(neig, val_accuracy['norm'], label = 'CV Accuracy with norm')\n",
    "# plt.legend()\n",
    "# plt.title('k value VS Accuracy')\n",
    "# plt.xlabel('Number of Neighbors')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xticks(neig)\n",
    "# plt.show()\n",
    "\n",
    "# print('Best Accuracy with feature scaling:', bestAcc)\n",
    "# print('Best kNN classifier:', bestKnn)\n",
    "# print('Best scaling:', bestScaling)\n",
    "\n",
    "\n",
    "# # In[18]:\n",
    "\n",
    "\n",
    "# # predict on test\n",
    "# bestKnn.fit(X_norm, y)\n",
    "# submission = pd.DataFrame(bestKnn.predict(norm.transform(test_fill)))\n",
    "# print(submission.shape)\n",
    "# submission.columns = ['Solution']\n",
    "# submission['Id'] = np.arange(1,submission.shape[0]+1)\n",
    "# submission = submission[['Id', 'Solution']]\n",
    "# submission\n",
    "\n",
    "\n",
    "# # In[20]:\n",
    "\n",
    "\n",
    "# submission.to_csv('./data-science-london-scikit-learn/submission_with_scaling.csv', index=False)\n",
    "\n",
    "\n",
    "# # ## Feature Selection\n",
    "\n",
    "# # In[21]:\n",
    "\n",
    "\n",
    "# #correlation map\n",
    "# f,ax = plt.subplots(figsize=(18, 18))\n",
    "# sns.heatmap(pd.DataFrame(X_std).corr(), annot=True, linewidths=.5, fmt='.1f',ax=ax)\n",
    "\n",
    "\n",
    "# # In[22]:\n",
    "\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import f1_score,confusion_matrix\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # split data train 70 % and val 30 %\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_std, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# #random forest classifier with n_estimators=10 (default)\n",
    "# clf_rf = RandomForestClassifier(random_state=43)      \n",
    "# clr_rf = clf_rf.fit(X_train,y_train)\n",
    "\n",
    "# ac = accuracy_score(y_val,clf_rf.predict(X_val))\n",
    "# print('Accuracy is: ',ac)\n",
    "# cm = confusion_matrix(y_val, clf_rf.predict(X_val))\n",
    "# sns.heatmap(cm, annot=True,fmt=\"d\")\n",
    "\n",
    "\n",
    "# # In[23]:\n",
    "\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.feature_selection import RFECV\n",
    "\n",
    "# kfold = 10\n",
    "# bestSVC = None\n",
    "# bestAcc = 0.0\n",
    "# val_accuracy = []\n",
    "# cv_range = np.arange(5, 11)\n",
    "# n_feature = []\n",
    "# for cv in cv_range:\n",
    "#     # Create the RFE object and compute a cross-validated score.\n",
    "#     svc = SVC(kernel=\"linear\")\n",
    "#     # The \"accuracy\" scoring is proportional to the number of correct\n",
    "#     # classifications\n",
    "#     rfecv = RFECV(estimator=svc, step=1, cv=cv, scoring='accuracy')\n",
    "#     rfecv.fit(X_std, y)\n",
    "\n",
    "#     # print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "#     # print('Best features :', pd.DataFrame(X_train).columns[rfecv.support_])\n",
    "\n",
    "#     # Model complexity\n",
    "#     val_accuracy += [np.mean(cross_val_score(svc, X_std[:, rfecv.support_], y, cv=kfold))]\n",
    "#     n_feature.append(rfecv.n_features_)\n",
    "#     if val_accuracy[-1] > bestAcc:\n",
    "#         bestAcc = val_accuracy[-1]\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=[13,8])\n",
    "# plt.plot(cv_range, val_accuracy, label = 'CV Accuracy')\n",
    "# for i in range(len(cv_range)):\n",
    "#     plt.annotate(str(n_feature[i]), xy=(cv_range[i],val_accuracy[i]))\n",
    "# plt.legend()\n",
    "# plt.title('Cross Validation Accuracy')\n",
    "# plt.xlabel('k fold')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.show()\n",
    "\n",
    "# print('Best Accuracy with feature scaling and RFECV:', bestAcc)\n",
    "\n",
    "\n",
    "# # In[24]:\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# #import sklearn as sk\n",
    "# #import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "\n",
    "# #from sklearn.linear_model import LogisticRegression\n",
    "# #from sklearn.linear_model import Perceptron\n",
    "# #from sklearn import tree\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# #from sklearn import svm\n",
    "\n",
    "# #### READING OUR GIVEN DATA INTO PANDAS DATAFRAME ####\n",
    "# x_train = train\n",
    "# y_train = trainLabel\n",
    "# x_test = test\n",
    "# x_train = np.asarray(x_train)\n",
    "# y_train = np.asarray(y_train)\n",
    "# x_test = np.asarray(x_test)\n",
    "# y_train = y_train.ravel()\n",
    "# print('training_x Shape:',x_train.shape,',training_y Shape:',y_train.shape, ',testing_x Shape:',x_test.shape)\n",
    "\n",
    "# #Checking the models\n",
    "# x_all = np.r_[x_train,x_test]\n",
    "# print('x_all shape :',x_all.shape)\n",
    "\n",
    "# #### USING THE GAUSSIAN MIXTURE MODEL ####\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "# lowest_bic = np.infty\n",
    "# bic = []\n",
    "# n_components_range = range(1, 7)\n",
    "# cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "# for cv_type in cv_types:\n",
    "#     for n_components in n_components_range:\n",
    "#         # Fit a mixture of Gaussians with EM\n",
    "#         gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n",
    "#         gmm.fit(x_all)\n",
    "#         bic.append(gmm.aic(x_all))\n",
    "#         if bic[-1] < lowest_bic:\n",
    "#             lowest_bic = bic[-1]\n",
    "#             best_gmm = gmm\n",
    "            \n",
    "# best_gmm.fit(x_all)\n",
    "# x_train = best_gmm.predict_proba(x_train)\n",
    "# x_test = best_gmm.predict_proba(x_test)\n",
    "\n",
    "\n",
    "# #### TAKING ONLY TWO MODELS FOR KEEPING IT SIMPLE ####\n",
    "# knn = KNeighborsClassifier()\n",
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# param_grid = dict( )\n",
    "# #### GRID SEARCH for BEST TUNING PARAMETERS FOR KNN #####\n",
    "# grid_search_knn = GridSearchCV(knn,param_grid=param_grid,cv=10,scoring='accuracy').fit(x_train,y_train)\n",
    "# print('best estimator KNN:',grid_search_knn.best_estimator_,'Best Score', grid_search_knn.best_estimator_.score(x_train,y_train))\n",
    "# knn_best = grid_search_knn.best_estimator_\n",
    "\n",
    "# #### GRID SEARCH for BEST TUNING PARAMETERS FOR RandomForest #####\n",
    "# grid_search_rf = GridSearchCV(rf, param_grid=dict( ), verbose=3,scoring='accuracy',cv=10).fit(x_train,y_train)\n",
    "# print('best estimator RandomForest:',grid_search_rf.best_estimator_,'Best Score', grid_search_rf.best_estimator_.score(x_train,y_train))\n",
    "# rf_best = grid_search_rf.best_estimator_\n",
    "\n",
    "\n",
    "# knn_best.fit(x_train,y_train)\n",
    "# print(knn_best.predict(x_test)[0:10])\n",
    "# rf_best.fit(x_train,y_train)\n",
    "# print(rf_best.predict(x_test)[0:10])\n",
    "\n",
    "# #### SCORING THE MODELS ####\n",
    "# print('Score for KNN :',cross_val_score(knn_best,x_train,y_train,cv=10,scoring='accuracy').mean())\n",
    "# print('Score for Random Forest :',cross_val_score(rf_best,x_train,y_train,cv=10,scoring='accuracy').max())\n",
    "\n",
    "# ### IN CASE WE WERE USING MORE THAN ONE CLASSIFIERS THEN VOTING CLASSIFIER CAN BE USEFUL ###\n",
    "# #clf = VotingClassifier(\n",
    "# #\t\testimators=[('knn_best',knn_best),('rf_best',rf_best)],\n",
    "# #\t\t#weights=[871856020222,0.907895269918]\n",
    "# #\t)\n",
    "# #clf.fit(x_train,y_train)\n",
    "# #print clf.predict(x_test)[0:10]\n",
    "\n",
    "# ##### FRAMING OUR SOLUTION #####\n",
    "# knn_best_pred = pd.DataFrame(knn_best.predict(x_test))\n",
    "# rf_best_pred = pd.DataFrame(rf_best.predict(x_test))\n",
    "# #voting_clf_pred = pd.DataFrame(clf.predict(x_test))\n",
    "\n",
    "# knn_best_pred.index += 1\n",
    "# rf_best_pred.index += 1\n",
    "# #voting_clf_pred.index += 1\n",
    "\n",
    "# rf_best_pred.columns = ['Solution']\n",
    "# rf_best_pred['Id'] = np.arange(1,rf_best_pred.shape[0]+1)\n",
    "# rf_best_pred = rf_best_pred[['Id', 'Solution']]\n",
    "# print(rf_best_pred)\n",
    "\n",
    "# #knn_best_pred.to_csv('knn_best_pred.csv')\n",
    "# rf_best_pred.to_csv('./data-science-london-scikit-learn/Submission_rf.csv', index=False)\n",
    "# #voting_clf_pred.to_csv('voting_clf_pred.csv')\n",
    "\n",
    "\n",
    "# # ## Neural Network\n",
    "\n",
    "# # In[28]:\n",
    "\n",
    "\n",
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load in \n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# from keras.layers import Conv2D,Dropout,Dense,Flatten\n",
    "# from keras.models import Sequential\n",
    "\n",
    "# # Input data files are available in the \"../input/\" directory.\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# import os\n",
    "# print(os.listdir(\"./data-science-london-scikit-learn\"))\n",
    "\n",
    "\n",
    "# # In[29]:\n",
    "\n",
    "\n",
    "# train = pd.read_csv(\"./data-science-london-scikit-learn/train.csv\", header=None)\n",
    "\n",
    "\n",
    "# # In[30]:\n",
    "\n",
    "\n",
    "# test = pd.read_csv(\"./data-science-london-scikit-learn/test.csv\", header=None)\n",
    "\n",
    "\n",
    "# # In[31]:\n",
    "\n",
    "\n",
    "# train_labels = pd.read_csv(\"./data-science-london-scikit-learn/trainLabels.csv\", header=None)\n",
    "\n",
    "\n",
    "# # In[32]:\n",
    "\n",
    "\n",
    "# train.shape\n",
    "\n",
    "\n",
    "# # In[33]:\n",
    "\n",
    "\n",
    "# test.shape\n",
    "\n",
    "\n",
    "# # In[34]:\n",
    "\n",
    "\n",
    "# y = train_labels\n",
    "# X = train\n",
    "\n",
    "\n",
    "# # In[35]:\n",
    "\n",
    "\n",
    "# # Importing the Keras libraries and packages\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# #Initializing Neural Network\n",
    "# classifier = Sequential()\n",
    "\n",
    "# # Adding the input layer and the first hidden layer\n",
    "# classifier.add(Dense(output_dim = 128, init = 'uniform', activation = 'relu', input_dim = 40))\n",
    "# # Adding the second hidden layer\n",
    "# classifier.add(Dense(output_dim = 64, init = 'uniform', activation = 'relu'))\n",
    "# # Adding the output layer\n",
    "# classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# # Compiling Neural Network\n",
    "# classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "# # In[36]:\n",
    "\n",
    "\n",
    "# # Fitting our model \n",
    "# classifier.fit(X, y, batch_size = 10, nb_epoch = 500)\n",
    "\n",
    "\n",
    "# # In[37]:\n",
    "\n",
    "\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# # Predicting the Test set results\n",
    "# y_pred = classifier.predict(X)\n",
    "# val_mae = mean_absolute_error(y_pred, y)\n",
    "\n",
    "\n",
    "# # In[38]:\n",
    "\n",
    "\n",
    "# val_mae\n",
    "\n",
    "\n",
    "# # In[40]:\n",
    "\n",
    "\n",
    "# test_y = classifier.predict(test)\n",
    "# test_y = np.array(test_y, dtype=int)\n",
    "# index_column = np.arange(1,9001)\n",
    "# myDataFrame = pd.DataFrame(test_y,columns=['Solution'],dtype=int)\n",
    "# myDataFrame['Id'] = index_column\n",
    "# myDataFrame.index.name = 'Id'\n",
    "# myDataFrame.set_index(index_column)\n",
    "\n",
    "\n",
    "# # In[42]:\n",
    "\n",
    "\n",
    "# myDataFrame.to_csv(\"./data-science-london-scikit-learn/submission_NN.csv\", index=False)\n",
    "\n",
    "\n",
    "# # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
